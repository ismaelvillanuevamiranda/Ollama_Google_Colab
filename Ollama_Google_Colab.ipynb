{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import time\n",
        "\n",
        "\n",
        "class OllamaManager:\n",
        "    def __init__(self, model=\"llama3.1:latest\", base_url=\"http://localhost:11434/v1\"):\n",
        "        \"\"\"\n",
        "        Initializes the OllamaManager:\n",
        "        - Installs Ollama\n",
        "        - Starts Ollama server\n",
        "        - Pulls the default Llama model\n",
        "        - Sets up environment variables\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.base_url = base_url\n",
        "        self.initialized = False\n",
        "\n",
        "        self.setup()\n",
        "\n",
        "    def setup(self):\n",
        "        \"\"\"Sets up Ollama, pulls the model, and lists installed models.\"\"\"\n",
        "        if not self.initialized:\n",
        "            self.install_ollama()\n",
        "            self.start_ollama_server()\n",
        "            self.pull_model(self.model)\n",
        "            self.list_installed_models()\n",
        "            self.agent = llmAgent(base_url=self.base_url, model=self.model, api_key=\"none\")\n",
        "            self.initialized = True\n",
        "        else:\n",
        "            print(\"ðŸŸ¢ Setpu has already been initialized.\")\n",
        "\n",
        "    def install_ollama(self):\n",
        "        \"\"\"Installs Ollama on the system.\"\"\"\n",
        "        print(\"ðŸ“Œ Installing Ollama...\")\n",
        "        os.system(\"curl -fsSL https://ollama.com/install.sh | sh\")\n",
        "        os.system(\"pip install openai pandas requests\")\n",
        "        print(\"âœ… Installation completed.\")\n",
        "\n",
        "    def start_ollama_server(self):\n",
        "        \"\"\"Starts the Ollama servre.\"\"\"\n",
        "        print(\"ðŸ“Œ Setting up environment variables...\")\n",
        "        os.environ[\"OLLAMA_HOST\"] = \"0.0.0.0\"\n",
        "        os.environ[\"no_proxy\"] = \"localhost,127.0.0.1\"\n",
        "\n",
        "        print(\"ðŸ“Œ Starting Ollama server in the background...\")\n",
        "        os.system(\"nohup ollama serve &\")\n",
        "        print(\"ðŸ•’ Waiting for the server to start...\")\n",
        "        time.sleep(5)  # wait some time for the server to start\n",
        "\n",
        "    def pull_model(self, model_name):\n",
        "        \"\"\"Pulls the specified Llama model.\"\"\"\n",
        "        print(f\"ðŸ“Œ Pulling model: {model_name}...\")\n",
        "        os.system(f\"ollama pull {model_name}\")\n",
        "        print(f\"âœ… Model {model_name} pulled successfully.\")\n",
        "\n",
        "    def list_installed_models(self):\n",
        "        \"\"\"Lists all installed models in Ollama.\"\"\"\n",
        "        print(\"ðŸ“Œ Listing installed models...\")\n",
        "        !ollama list\n",
        "\n",
        "    def change_model(self, new_model):\n",
        "        \"\"\"Changes the model without re-running setup.\"\"\"\n",
        "        print(f\"ðŸ”„ Changing model to: {new_model}...\")\n",
        "        self.model = new_model\n",
        "        self.pull_model(new_model)\n",
        "        self.agent = llmAgent(base_url=self.base_url, model=self.model, api_key=\"none\")\n",
        "        print(f\"âœ… Model switched to: {new_model}\")\n",
        "\n",
        "\n",
        "class llmAgent:\n",
        "    def __init__(self, base_url, api_key, model, temperature=0):\n",
        "        \"\"\"\n",
        "        Initializes the LLM Agent for interacting with the local model.\n",
        "        \"\"\"\n",
        "        self.client = OpenAI(\n",
        "            base_url=base_url,\n",
        "            api_key=api_key\n",
        "        )\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def ask_question(self, question, system_role_content, temperature=None):\n",
        "        \"\"\"\n",
        "        Asks a question to the model and returns the response.\n",
        "        \"\"\"\n",
        "        if temperature is None:\n",
        "            temperature = self.temperature\n",
        "\n",
        "        response = self.client.chat.completions.create(\n",
        "            model=self.model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_role_content},\n",
        "                {\"role\": \"user\", \"content\": question},\n",
        "            ],\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# -------------------- Example Usage --------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize OllamaManager\n",
        "    ollama = OllamaManager(model=\"llama3.1:latest\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AbHjvZ8Wt-C",
        "outputId": "1f6b5a24-8831-4898-f133-6b01ae2a6e3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Œ Installing Ollama...\n",
            "âœ… Installation completed.\n",
            "ðŸ“Œ Setting up environment variables...\n",
            "ðŸ“Œ Starting Ollama server in the background...\n",
            "ðŸ•’ Waiting for the server to start...\n",
            "ðŸ“Œ Pulling model: llama3.1:latest...\n",
            "âœ… Model llama3.1:latest pulled successfully.\n",
            "ðŸ“Œ Listing installed models...\n",
            "NAME               ID              SIZE      MODIFIED               \n",
            "llama3.1:latest    46e0c10c039e    4.9 GB    Less than a second ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ollama.list_installed_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMpxG8uuYj2E",
        "outputId": "211fd061-2606-45d8-81f4-69b838be4b5f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Œ Listing installed models...\n",
            "NAME               ID              SIZE      MODIFIED      \n",
            "llama3.1:latest    46e0c10c039e    4.9 GB    7 seconds ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# role = \"You are a highly skilled mathematics assistant, capable of solving problems, explaining concepts, and providing step-by-step solutions in various mathematical domains, including algebra, calculus, geometry, statistics, and more.\"\n",
        "# role = \"You are a mathematics assistant specializing in solving mathematical problems across different domains. You provide step-by-step explanations, visual breakdowns when applicable, and concise yet thorough solutions. Ensure clarity and correctness in your responses.\"\n",
        "# role = \"You are a mathematics assistant trained to solve complex mathematical problems, assist in theorem proving, and provide in-depth explanations across topics such as real analysis, number theory, linear algebra, and differential equations. Present rigorous justifications for solutions and use LaTeX formatting where necessary.\"\n",
        "# role = \"You are an expert mathematics tutor, assisting students by explaining mathematical concepts clearly and intuitively. Adapt your explanations based on the learner's level, provide real-world examples where possible, and guide students through problem-solving with a step-by-step approach.\"\n",
        "role = \"You are a logical reasoning and mathematics assistant. Your goal is to solve word problems with a step-by-step approach, ensuring clarity, correctness, and logical consistency.\"\n",
        "\n",
        "query = \"If I hang 5 shirts outside and it takes them 5 hours to dry, how long would it take to dry 30 shirts?\"\n",
        "\n",
        "\n",
        "ollama.change_model(\"llama3.1:latest\")\n",
        "answer = ollama.agent.ask_question(query, role)\n",
        "print(\"\\nðŸ”¹ Response from llama3.1:\")\n",
        "print(answer)\n",
        "\n",
        "\n",
        "ollama.change_model(\"mistral:7b\")\n",
        "answer = ollama.agent.ask_question(query, role)\n",
        "print(\"\\nðŸ”¹ Response from mistral:7b:\")\n",
        "print(answer)\n",
        "\n",
        "\n",
        "ollama.change_model(\"llama3.2:latest\")\n",
        "answer = ollama.agent.ask_question(query, role)\n",
        "print(\"\\nðŸ”¹ Response from llama3.2:latest:\")\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWqkyhXbX68-",
        "outputId": "deda35d5-d05b-4246-d730-d8be275003a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ”„ Changing model to: llama3.1:latest...\n",
            "ðŸ“Œ Pulling model: llama3.1:latest...\n",
            "âœ… Model llama3.1:latest pulled successfully.\n",
            "âœ… Model switched to: llama3.1:latest\n",
            "\n",
            "ðŸ”¹ Response from llama3.1:\n",
            "Let's break down the problem step by step:\n",
            "\n",
            "1. We know that 5 shirts take 5 hours to dry.\n",
            "2. To find out how long it takes for one shirt to dry, we can divide the total time (5 hours) by the number of shirts (5). This gives us:\n",
            "   Time per shirt = Total time / Number of shirts\n",
            "                    = 5 hours / 5 shirts\n",
            "                    = 1 hour per shirt\n",
            "\n",
            "3. Now that we know it takes 1 hour for one shirt to dry, we can find out how long it would take to dry 30 shirts by multiplying the number of shirts (30) by the time per shirt (1 hour):\n",
            "   Time to dry 30 shirts = Number of shirts Ã— Time per shirt\n",
            "                           = 30 shirts Ã— 1 hour/shirt\n",
            "                           = 30 hours\n",
            "\n",
            "Therefore, it would take 30 hours for 30 shirts to dry.\n",
            "ðŸ”„ Changing model to: mistral:7b...\n",
            "ðŸ“Œ Pulling model: mistral:7b...\n",
            "âœ… Model mistral:7b pulled successfully.\n",
            "âœ… Model switched to: mistral:7b\n",
            "\n",
            "ðŸ”¹ Response from mistral:7b:\n",
            " To solve this problem, let's first find out the drying rate for one shirt. We know that 5 shirts take 5 hours to dry. So, if we assume that all shirts are drying at the same rate, then each shirt takes 5 hours / 5 shirts = 1 hour to dry.\n",
            "\n",
            "Now, since we want to find out how long it would take to dry 30 shirts, we can simply multiply the time it takes for one shirt to dry by the number of shirts:\n",
            "\n",
            "Time to dry 30 shirts = Time per shirt * Number of shirts\n",
            "                     = 1 hour * 30\n",
            "                     = 30 hours\n",
            "\n",
            "So, it would take 30 hours to dry 30 shirts if they are all drying at the same rate.\n",
            "ðŸ”„ Changing model to: llama3.2:latest...\n",
            "ðŸ“Œ Pulling model: llama3.2:latest...\n",
            "âœ… Model llama3.2:latest pulled successfully.\n",
            "âœ… Model switched to: llama3.2:latest\n",
            "\n",
            "ðŸ”¹ Response from llama3.2:latest:\n",
            "To find the answer, let's analyze the situation:\n",
            "\n",
            "1. It takes 5 shirts 5 hours to dry.\n",
            "2. We want to know how long it will take for 30 shirts to dry.\n",
            "\n",
            "Since the number of shirts is increasing by a factor of 6 (from 5 to 30), we can assume that the drying time will also increase proportionally.\n",
            "\n",
            "Let's set up a proportion:\n",
            "\n",
            "(5 shirts) / (5 hours) = (30 shirts) / x hours\n",
            "\n",
            "To solve for x, we can cross-multiply and simplify:\n",
            "\n",
            "5x = 30 Ã— 5\n",
            "5x = 150\n",
            "x = 150 / 5\n",
            "x = 30\n",
            "\n",
            "So, it would take 30 hours to dry 30 shirts.\n",
            "\n",
            "However, this seems counterintuitive. If it takes 5 hours for 5 shirts to dry, why does it take 6 times as long (30 hours) for 30 shirts? The answer lies in the fact that drying time is not directly proportional to the number of shirts. Other factors like air circulation, temperature, and humidity can affect the drying time.\n",
            "\n",
            "In reality, the actual drying time may be shorter or longer than our calculated value. To get a more accurate estimate, we would need to consider these additional factors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ollama.list_installed_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VawXe1pTnLxf",
        "outputId": "22bff114-ba19-4c2b-a511-6ce3a7e89203"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“Œ Listing installed models...\n",
            "NAME               ID              SIZE      MODIFIED       \n",
            "llama3.2:latest    a80c4f17acd5    2.0 GB    16 seconds ago    \n",
            "mistral:7b         f974a74358d6    4.1 GB    2 minutes ago     \n",
            "llama3.1:latest    46e0c10c039e    4.9 GB    4 minutes ago     \n"
          ]
        }
      ]
    }
  ]
}